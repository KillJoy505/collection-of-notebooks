{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuickFaceSwap",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/QuickFaceSwap_beta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFf145OKhrZh"
      },
      "source": [
        "<b><font color=\"black\" size=\"+4\">QuickFaceSwap v.1.2</font></b>\n",
        "\n",
        "version with another enhancer: [QFS v.1.1](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/QuickFaceSwap.ipynb)\n",
        "\n",
        "<b><font color=\"black\" size=\"+2\">Based on:</font></b>\n",
        "\n",
        "**GitHub repositories**: [SimSwap](https://github.com/neuralchen/SimSwap), [GPEN](https://github.com/yangxy/GPEN)\n",
        "\n",
        "**Articles**: [SimSwap: An Efficient Framework For High Fidelity Face Swapping](https://arxiv.org/pdf/2106.06340v1.pdf), [GAN Prior Embedded Network for Blind Face Restoration in the Wild](https://arxiv.org/pdf/2105.06070.pdf)\n",
        "\n",
        "**Creators of SimSwap:** *Renwang Chen, Xuanhong Chen, Bingbing Ni, Yanhao Ge*\n",
        "\n",
        "**Creators of GPEN:** *Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang*\n",
        "\n",
        "<b><font color=\"black\" size=\"+2\">Colab created by:</font></b>\n",
        "\n",
        "GitHub: [@tg-bomze](https://github.com/tg-bomze),\n",
        "Telegram: [@bomze](https://t.me/bomze),\n",
        "Twitter: [@tg_bomze](https://twitter.com/tg_bomze).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "```\n",
        "(ENG) To get started, click on the button (where the red arrow indicates). After clicking, wait until the execution is complete.\n",
        "```\n",
        "```\n",
        "(RUS) Чтобы начать, поочередно нажимайте на кнопки (куда указывают красные стрелки), дожидаясь завершения выполнения каждого блока.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr_2fdxirkkb",
        "cellView": "form"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Install all necessary libraries</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Установить все необходимые библиотеки</font></b>\n",
        "\n",
        "#@markdown **Follow this link (перейдите по ссылке):**\n",
        "\n",
        "#@markdown **https://drive.google.com/drive/folders/1jV6_0FIMPC53FZ2HzZNJZGMe55bbu17R**\n",
        "\n",
        "#@markdown **1) right click on 'checkpoints' (правой кнопкой по checkpoints)**\n",
        "\n",
        "#@markdown **2) select 'Add shortcut to Drive (выберите Сохранить ярлык на Диск)**\n",
        "\n",
        "#@markdown ![](https://github.com/tg-bomze/collection-of-notebooks/raw/master/dfs.png)\n",
        "\n",
        "#@markdown **3) run this block and follow the further instructions (после этого запустите блок и следуйте инструкции)**\n",
        "\n",
        "#@markdown *Attention! If the weights have already been saved, then run this block and just mount Google Drive.*\n",
        "\n",
        "#@markdown *Внимание! Если веса уже сохранены, то можете сразу запустить блок и смонтировать Google Drive.*\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import cv2\n",
        "drive.mount('/content/drive')\n",
        "!pip install ffmpeg-python\n",
        "!pip install facexlib\n",
        "import ffmpeg\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/yangxy/GPEN\n",
        "!wget https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/RetinaFace-R50.pth && mv RetinaFace-R50.pth GPEN/weights/\n",
        "!wget https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/GPEN-512.pth && mv GPEN-512.pth GPEN/weights/\n",
        "!wget https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/GPEN-1024-Color.pth && mv GPEN-1024-Color.pth GPEN/weights/\n",
        "!pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\n",
        "!pip install pip install opencv-python\n",
        "!pip install ffmpeg-python\n",
        "%cd /content/GPEN\n",
        "!wget https://www.dropbox.com/s/0zwrook2mxxb4vc/modnet_photographic_portrait_matting.ckpt\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "!mkdir MODNet\n",
        "%cd /content/GPEN/MODNet\n",
        "!wget https://www.dropbox.com/s/4nwzaaq5x1tm2ze/MODNet.zip\n",
        "!unzip /content/GPEN/MODNet/MODNet.zip\n",
        "!rm -rf /content/GPEN/MODNet/MODNet.zip\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/neuralchen/SimSwap\n",
        "%cd /content/SimSwap\n",
        "!mkdir -p /usr/local/lib/python3.7/dist-packages/facexlib/weights\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "!rm -rf shape_predictor_68_face_landmarks.dat.bz2\n",
        "!mkdir arcface_model checkpoints\n",
        "%cd /content/SimSwap/checkpoints\n",
        "!cp /content/drive/MyDrive/checkpoint/checkpoints.zip ./\n",
        "!unzip /content/SimSwap/checkpoints/checkpoints.zip\n",
        "%cd /content/SimSwap/arcface_model\n",
        "!cp /content/drive/MyDrive/checkpoint/arcface_checkpoint.tar ./\n",
        "%cd /content/SimSwap\n",
        "\n",
        "path_to_frames = '/content/frames'\n",
        "path_to_result = '/content/result'\n",
        "path_to_final = '/content/result/final'\n",
        "!rm -rf $path_to_frames $path_to_result $path_to_final /content/sample_data\n",
        "!mkdir $path_to_frames $path_to_result $path_to_final\n",
        "\n",
        "clear_output()\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ZiRzhY7BYP",
        "cellView": "form"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Upload video</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Загрузить видео</font></b>\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import ffmpeg\n",
        "\n",
        "uploaded = files.upload()\n",
        "for vp in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=vp, length=len(uploaded[vp])))\n",
        "video_path = f'/content/SimSwap/{vp}'\n",
        "\n",
        "!rm -rf /content/frames/*\n",
        "!ffmpeg -y -i $video_path /content/audio.mp3\n",
        "!ffmpeg -i $video_path /content/frames/%7d.png\n",
        "\n",
        "fps_of_video = int(cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FPS))\n",
        "fps_of_video +=1\n",
        "\n",
        "if os.listdir('/content/frames'):\n",
        "  clear_output()\n",
        "  print('Done!')\n",
        "else: print('Error!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozr3wZfzlqIk",
        "cellView": "form"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Upload photo with face</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Загрузить фото с лицом</font></b>\n",
        "uploaded = files.upload()\n",
        "for pp in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=pp, length=len(uploaded[pp])))\n",
        "clear_output()\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLbOozniGi_H",
        "cellView": "form"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Prepare script</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Подготовить скрипт</font></b>\n",
        "%%writefile /content/SimSwap/test_video.py\n",
        "import torch\n",
        "import fractions\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import normalize\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import numpy as np\n",
        "import imageio\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import scipy\n",
        "import scipy.ndimage\n",
        "import dlib\n",
        "\n",
        "\n",
        "size = 224\n",
        "path_to_frames = '/content/frames'\n",
        "path_to_result = '/content/result'\n",
        "path_to_final = '/content/result/final'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Start. Please wait little bit...')\n",
        "\n",
        "sys.path.append(\"/content/GPEN\")\n",
        "os.chdir('/content/GPEN')\n",
        "from MODNet.models.modnet import MODNet\n",
        "import __init_paths\n",
        "from retinaface.retinaface_detection import RetinaFaceDetection\n",
        "from face_model.face_gan import FaceGAN\n",
        "from align_faces import warp_and_crop_face, get_reference_facial_points\n",
        "from skimage import transform as tf\n",
        "\n",
        "weight = \"/content/GPEN/modnet_photographic_portrait_matting.ckpt\"\n",
        "im_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "modnet = MODNet(backbone_pretrained=False)\n",
        "modnet = nn.DataParallel(modnet).cuda()\n",
        "modnet.load_state_dict(torch.load(weight))\n",
        "modnet.eval()\n",
        "\n",
        "def post_process(color, orig):\n",
        "  orig_yuv = cv2.cvtColor(orig, cv2.COLOR_BGR2YUV)\n",
        "  color_yuv = cv2.cvtColor(color, cv2.COLOR_BGR2YUV)\n",
        "  hires = np.copy(color_yuv)\n",
        "  hires[:, :, 1:3] = orig_yuv[:, :, 1:3]\n",
        "  return cv2.cvtColor(hires, cv2.COLOR_YUV2BGR)\n",
        "\n",
        "def increase_brightness(img, value=30):\n",
        "  hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  h, s, v = cv2.split(hsv)\n",
        "  lim = 255 - value\n",
        "  v[v > lim] = 255\n",
        "  v[v <= lim] += value\n",
        "  final_hsv = cv2.merge((h, s, v))\n",
        "  return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "def remove_bg(face, bg):\n",
        "  im = im_transform(bg)\n",
        "  im = im[None, :, :, :]\n",
        "  im_b, im_c, im_h, im_w = im.shape\n",
        "  im_rw = im_w - im_w % 32\n",
        "  im_rh = im_h - im_h % 32\n",
        "  im = F.interpolate(im, size=(im_rh, im_rw), mode='area')\n",
        "  _, _, matte = modnet(im.cuda(), True)\n",
        "  matte = F.interpolate(matte, size=(im_h, im_w), mode='area')\n",
        "  matte = matte[0][0].data.cpu().numpy()\n",
        "  matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2)\n",
        "  out = face.copy()\n",
        "  out[matte<0.5] = bg[matte<0.5]\n",
        "  return out\n",
        "\n",
        "class FaceEnhancement(object):\n",
        "    def __init__(self, base_dir='./', size=512, model=None, channel_multiplier=2):\n",
        "        self.facedetector = RetinaFaceDetection(base_dir)\n",
        "        self.facegan = FaceGAN(base_dir, size, model, channel_multiplier)\n",
        "        self.size = size\n",
        "        self.threshold = 0.9\n",
        "        self.mask = np.zeros((512, 512), np.float32)\n",
        "        cv2.rectangle(self.mask, (26, 26), (486, 486), (1, 1, 1), -1, cv2.LINE_AA)\n",
        "        self.mask = cv2.GaussianBlur(self.mask, (101, 101), 11)\n",
        "        self.mask = cv2.GaussianBlur(self.mask, (101, 101), 11)\n",
        "        self.kernel = np.array((\n",
        "                [0.0625, 0.125, 0.0625],\n",
        "                [0.125, 0.25, 0.125],\n",
        "                [0.0625, 0.125, 0.0625]), dtype=\"float32\")\n",
        "        # get the reference 5 landmarks position in the crop settings\n",
        "        default_square = True\n",
        "        inner_padding_factor = 0.25\n",
        "        outer_padding = (0, 0)\n",
        "        self.reference_5pts = get_reference_facial_points(\n",
        "                (self.size, self.size), inner_padding_factor, outer_padding, default_square)\n",
        "\n",
        "    def process(self, img):\n",
        "        facebs, landms = self.facedetector.detect(img)\n",
        "        orig_faces, enhanced_faces = [], []\n",
        "        height, width = img.shape[:2]\n",
        "        full_mask = np.zeros((height, width), dtype=np.float32)\n",
        "        full_img = np.zeros(img.shape, dtype=np.uint8)\n",
        "\n",
        "        for i, (faceb, facial5points) in enumerate(zip(facebs, landms)):\n",
        "            if faceb[4]<self.threshold: continue\n",
        "            fh, fw = (faceb[3]-faceb[1]), (faceb[2]-faceb[0])\n",
        "            facial5points = np.reshape(facial5points, (2, 5))\n",
        "            of, tfm_inv = warp_and_crop_face(img, facial5points, reference_pts=self.reference_5pts, crop_size=(self.size, self.size))\n",
        "            ef = self.facegan.process(of)\n",
        "            \n",
        "            orig_faces.append(of)\n",
        "            enhanced_faces.append(ef)\n",
        "            \n",
        "            tmp_mask = self.mask\n",
        "            tmp_mask = cv2.resize(tmp_mask, ef.shape[:2])\n",
        "            tmp_mask = cv2.warpAffine(tmp_mask, tfm_inv, (width, height), flags=3)\n",
        "            if min(fh, fw)<100: # gaussian filter for small faces\n",
        "                ef = cv2.filter2D(ef, -1, self.kernel)\n",
        "            tmp_img = cv2.warpAffine(ef, tfm_inv, (width, height), flags=3)\n",
        "            mask = tmp_mask - full_mask\n",
        "            full_mask[np.where(mask>0)] = tmp_mask[np.where(mask>0)]\n",
        "            full_img[np.where(mask>0)] = tmp_img[np.where(mask>0)]\n",
        "        full_mask = full_mask[:, :, np.newaxis]\n",
        "        img = cv2.convertScaleAbs(img*(1-full_mask) + full_img*full_mask)\n",
        "\n",
        "        return img, orig_faces, enhanced_faces\n",
        "\n",
        "faceenhancer = FaceEnhancement(size=512, model='GPEN-512', channel_multiplier=2)\n",
        "\n",
        "sys.path.append(\"/content/SimSwap\")\n",
        "os.chdir('/content/SimSwap')\n",
        "from models.models import create_model\n",
        "from options.test_options import TestOptions\n",
        "from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "transformer_Arcface = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "detransformer = transforms.Compose([\n",
        "        transforms.Normalize([0, 0, 0], [1/0.229, 1/0.224, 1/0.225]),\n",
        "        transforms.Normalize([-0.485, -0.456, -0.406], [1, 1, 1])\n",
        "    ])\n",
        "\n",
        "opt = TestOptions().parse()\n",
        "path_to_face = opt.pic_a_path\n",
        "start_epoch, epoch_iter = 1, 0\n",
        "torch.nn.Module.dump_patches = True\n",
        "model = create_model(opt)\n",
        "model.eval()\n",
        "\n",
        "def swapping(face_helper, img_path, latend_id, img_id, save_root):\n",
        "    img_name = os.path.basename(img_path)\n",
        "    basename, _ = os.path.splitext(img_name)\n",
        "    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "    face_helper.clean_all()\n",
        "\n",
        "    face_helper.read_image(input_img)\n",
        "    face_helper.get_face_landmarks_5(only_center_face=True, pad_blur=False)\n",
        "    save_crop_path = os.path.join(save_root, 'cropped_faces', img_name)\n",
        "    face_helper.align_warp_face(save_crop_path)\n",
        "\n",
        "    for idx, cropped_face in enumerate(face_helper.cropped_faces):\n",
        "      # Face swap\n",
        "      small_cropped_face = cv2.resize(cropped_face, (size,size))\n",
        "      pil_small_cropped_face = Image.fromarray(small_cropped_face).convert('RGB')\n",
        "      img_b = transformer(pil_small_cropped_face)\n",
        "      img_att = img_b.view(-1, img_b.shape[0], img_b.shape[1], img_b.shape[2])\n",
        "      img_att = img_att.cuda()\n",
        "\n",
        "      img_fake = model(img_id, img_att, latend_id, latend_id, True)\n",
        "      for i in range(img_id.shape[0]):\n",
        "          if i == 0:\n",
        "              row1 = img_id[i]\n",
        "              row2 = img_att[i]\n",
        "              row3 = img_fake[i]\n",
        "          else:\n",
        "              row1 = torch.cat([row1, img_id[i]], dim=2)\n",
        "              row2 = torch.cat([row2, img_att[i]], dim=2)\n",
        "              row3 = torch.cat([row3, img_fake[i]], dim=2)\n",
        "\n",
        "      full = row3.detach()\n",
        "      full = full.permute(1, 2, 0)\n",
        "      output = full.to('cpu')\n",
        "      output = np.array(output)\n",
        "      output = output[..., ::-1]*255\n",
        "      final_face = cv2.resize(output, (512,512))\n",
        "\n",
        "      # Enhance\n",
        "      name = os.path.basename(img_path)\n",
        "      cv2.imwrite(f'/content/result/final/{name}', final_face)\n",
        "      im = cv2.imread(f'/content/result/final/{name}', cv2.IMREAD_COLOR)\n",
        "      final_face, orig_faces, enhanced_faces = faceenhancer.process(im)\n",
        "      final_face = cv2.cvtColor(final_face, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "      out = remove_bg(final_face, cropped_face)\n",
        "      out = post_process(out, cropped_face)\n",
        "      face_helper.add_restored_face(out)\n",
        "\n",
        "    face_helper.get_inverse_affine(None)\n",
        "    save_restore_path = os.path.join(save_root, 'restored_imgs', img_name)\n",
        "    face_helper.paste_faces_to_input_image(save_restore_path)\n",
        "\n",
        "source_helper = FaceRestoreHelper(face_size=512, upscale_factor=1, crop_ratio=(1, 1), det_model='retinaface_resnet50', save_ext='png')\n",
        "\n",
        "source_helper.read_image(path_to_face)\n",
        "source_helper.get_face_landmarks_5(only_center_face=True, pad_blur=False)\n",
        "save_crop_path = os.path.join(path_to_result, 'cropped_faces', os.path.basename(path_to_face))\n",
        "source_helper.align_warp_face(save_crop_path)\n",
        "\n",
        "path_to_croped_src = '/content/result/cropped_faces/'\n",
        "path_to_cropped_src = path_to_croped_src+os.listdir(path_to_croped_src)[0]\n",
        "cropped_src = Image.open(path_to_cropped_src).convert('RGB')\n",
        "small_cropped_src = cropped_src.resize((size,size), Image.ANTIALIAS)\n",
        "\n",
        "img_a = transformer_Arcface(small_cropped_src)\n",
        "img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])\n",
        "img_id = img_id.cuda()\n",
        "img_id_downsample = F.interpolate(img_id, scale_factor=0.5)\n",
        "latend_id = model.netArc(img_id_downsample)\n",
        "latend_id = latend_id.detach().to('cpu')\n",
        "latend_id = latend_id/np.linalg.norm(latend_id,axis=1,keepdims=True)\n",
        "latend_id = latend_id.to('cuda')\n",
        "\n",
        "print('---------------------')\n",
        "print('--- Face Swapping ---')\n",
        "print('---------------------')\n",
        "\n",
        "face_helper = FaceRestoreHelper(face_size=512, upscale_factor=1, crop_ratio=(1, 1), det_model='retinaface_resnet50', save_ext='png')\n",
        "img_list = sorted(glob.glob(os.path.join(path_to_frames, '*')))\n",
        "for img_path in tqdm(img_list):\n",
        "  swapping(face_helper, img_path, latend_id, img_id, path_to_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdmbPNEZ32km",
        "cellView": "form"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Process</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Стартуем</font></b>\n",
        "path_to_face_photo = f'/content/SimSwap/{pp}'\n",
        "!rm -rf /content/result/cropped_faces/* /content/result/restored_imgs/* /content/result/cropped_faces/.ipynb_checkpoints /content/result/final/*\n",
        "!python test_video.py --Arc_path /content/drive/MyDrive/checkpoint/arcface_checkpoint.tar --pic_a_path $path_to_face_photo\n",
        "if os.listdir('/content/result/restored_imgs'):\n",
        "  clear_output()\n",
        "  print('Done!')\n",
        "else: print('Error!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUin8V0vAirE",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8523a70d-4b5e-4a59-d66e-4bb83a2711ee"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Merge frames into video</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Объединяем кадры в видео</font></b>\n",
        "video_type = \"good for editing\" #@param [\"good for sharing\", \"good for editing\"]\n",
        "if video_type == \"good for sharing\":\n",
        "  !ffmpeg -y -framerate $fps_of_video -pattern_type glob -i '/content/result/restored_imgs/*.png' -i /content/audio.mp3 -c:a copy -shortest -c:v libx264 -r $fps_of_video -pix_fmt yuv420p /content/swapped.mp4\n",
        "else:\n",
        "  !ffmpeg -y -r $fps_of_video -i /content/result/restored_imgs/%7d.png -i /content/audio.mp3 /content/swapped.mp4\n",
        "clear_output()\n",
        "files.download('/content/swapped.mp4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8aef3e57-8289-4532-b807-94f4e9cbf3e9\", \"swapped.mp4\", 4636296)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l30Kg3KIUZK4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "\n",
        "`Done` - Increase the brightness of the face\n",
        "\n",
        "`Done` -   Make a round mask\n",
        "\n",
        "`Done` -   New enhancer\n",
        "\n",
        "`In process` - Make the right Frames2Video\n",
        "\n"
      ]
    }
  ]
}